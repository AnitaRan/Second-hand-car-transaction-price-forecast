{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve,validation_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier,callback\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/大数据挖掘/比赛/天池-二手车交易价格预测/'\n",
    "train_data = pd.read_csv(path + 'used_car_train_20200313.csv',sep=' ')\n",
    "test_data = pd.read_csv(path + 'used_car_testB_20200421.csv',sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_feature(train_data,test_data):\n",
    "    # 处理目标price长尾分布问题\n",
    "    train_data['price'] = np.log1p(train_data['price'])\n",
    "    \n",
    "    # 合并数据，方便处理和构造特征\n",
    "    train_data['train']=1\n",
    "    test_data['train']=0\n",
    "    data = pd.concat([train_data,test_data],ignore_index=True)\n",
    "    \n",
    "    # 数据预处理\n",
    "    data = data_preprocessing(data)\n",
    "    \n",
    "    # 特征工程\n",
    "    data = feature_engineering(data,train_data)\n",
    "    \n",
    "    # 筛选特征\n",
    "#     data = select_feature(data)    \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def data_preprocessing(data):\n",
    "    # 处理无用值，数据过于稀疏，'name'\n",
    "    data['name_count'] = data.groupby(['name'])['SaleID'].transform('count')\n",
    "    del data['name']\n",
    "    \n",
    "    # 处理无用值，数据偏斜严重,'seller','offerType'\n",
    "    del data['seller']\n",
    "    del data['offerType']\n",
    "    \n",
    "    # 处理异常值'notRepairedDamage'，将'-'转化成nan\n",
    "    data['notRepairedDamage'] = data['notRepairedDamage'].astype('str').apply(lambda x:x if x!='-' else None).astype('float32')\n",
    "    \n",
    "    # 处理异常值，截断'power''v_13' 'v_14'\n",
    "    data['power'][data['power']>600] = 600\n",
    "    data['power'][data['power']<1] = 1\n",
    "    data['v_13'][data['v_13']>6] = 6\n",
    "    data['v_14'][data['v_14']>4] = 4\n",
    "    \n",
    "    # 用众数填充缺失值\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    null_col = ['bodyType','fuelType','gearbox','notRepairedDamage','model']\n",
    "    si = SimpleImputer(strategy='most_frequent').fit(data[null_col])\n",
    "    data[null_col] = si.transform(data[null_col])\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def feature_engineering(data,train_data):\n",
    "    # 时间类特征 'regDate','creatDate'\n",
    "    data['creatDate'] = data['creatDate'].apply(date_process)\n",
    "    data['regDate'] = data['regDate'].apply(date_process)\n",
    "#     data['creat_year'] = data['creatDate'].dt.year\n",
    "#     data['creat_month'] = data['creatDate'].dt.month\n",
    "#     data['creat_day'] = data['creatDate'].dt.day\n",
    "    data['reg_year'] = data['regDate'].dt.year\n",
    "#     data['reg_month'] = data['regDate'].dt.month\n",
    "#     data['reg_day'] = data['regDate'].dt.day\n",
    "    data['days'] = (data['creatDate'] - data['regDate']).dt.days\n",
    "    data['years'] = round(data['days'] / 365,1)\n",
    "    bins = [0,1,2,3,5,8,10,15,20,30]\n",
    "    data['years_bin'] = pd.cut(data['years'],bins,labels=False)\n",
    "    del data['creatDate']\n",
    "    del data['regDate']\n",
    "    del data['days']\n",
    "    \n",
    "    # 地区类特征 'regionCode'\n",
    "    data['city'] = data['regionCode'].apply(lambda x:str(x)[:2]).astype('int32')\n",
    "#     data['region_count'] = data.groupby(['regionCode'])['SaleID'].transform('count')\n",
    "    del data['SaleID']\n",
    "    \n",
    "    # 可分类特征，数据分桶 'power'\n",
    "    bins = [i*10 for i in range(61)]\n",
    "    data['power_bin'] = pd.cut(data['power'],bins,labels=False)\n",
    "    \n",
    "    \n",
    "    # 可分类特征组合，与目标price组合\n",
    "    data = feature_merge(data,train_data,'brand')\n",
    "    data = feature_merge(data,train_data,'model')\n",
    "    data = feature_merge(data,train_data,'kilometer')\n",
    "         \n",
    "    \n",
    "    # 其他可分类特征组合,与目标price组合 ('years_bin','price') ('power_bin','price')\n",
    "    feat1 = 'years_bin'\n",
    "    data_gb = data.groupby(feat1)\n",
    "    all_infos = {}\n",
    "    for key,value in data_gb:\n",
    "        info = {}\n",
    "        value = value[value['price']>0]\n",
    "        info[feat1 +'_amount'] = len(value)\n",
    "        info[feat1 +'_price_max'] = value.price.max()\n",
    "        info[feat1 +'_price_min'] = value.price.min()\n",
    "        info[feat1 +'_price_median'] = value.price.median()\n",
    "        info[feat1 +'_price_mean'] = value.price.mean()\n",
    "        info[feat1 +'_price_std'] = value.price.std()\n",
    "        all_infos[key] = info\n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':feat1})\n",
    "    data = data.merge(df,how='left',on=feat1)\n",
    "    \n",
    "    feat2 = 'power_bin'\n",
    "    data_gb = data.groupby(feat2)\n",
    "    all_infos = {}\n",
    "    for key,value in data_gb:\n",
    "        info = {}\n",
    "        value = value[value['price']>0]\n",
    "        info[feat1 +'_amount'] = len(value)\n",
    "        info[feat1 +'_price_max'] = value.price.max()\n",
    "        info[feat1 +'_price_min'] = value.price.min()\n",
    "        info[feat1 +'_price_median'] = value.price.median()\n",
    "        info[feat1 +'_price_mean'] = value.price.mean()\n",
    "        info[feat1 +'_price_std'] = value.price.std()\n",
    "        all_infos[key] = info\n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':feat2})\n",
    "    data = data.merge(df,how='left',on=feat2)\n",
    "         \n",
    "    \n",
    "    # 匿名特征交叉组合\n",
    "    v_list = ['v_0','v_2', 'v_3', 'v_5', 'v_6',  'v_8',  'v_10', 'v_11', 'v_12']\n",
    "    for i in v_list:\n",
    "        for j in range(15):\n",
    "            data['new_'+ i +'*'+str(j)] = data[i] * data['v_'+str(j)]\n",
    "    for i in v_list:\n",
    "        for j in range(15):\n",
    "            data['new_'+ i + '+' + str(j)] = data[i] + data['v_'+str(j)]\n",
    "    for i in v_list:\n",
    "        for j in range(15):\n",
    "            data['new_'+ i + '-' + str(j)] = data[i] - data['v_'+str(j)]\n",
    "    for i in v_list:\n",
    "        data['new_' + i + '*years'] = data[i] * data['years']\n",
    "    \n",
    "    \n",
    "    # 数据压缩，通过调整数据类型，减少数据在内存中占用的空间\n",
    "    data = reduce_mem_usage(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def date_process(date):\n",
    "    year = int(str(date)[:4])\n",
    "    month = int(str(date)[4:6])\n",
    "    day = int(str(date)[6:8])\n",
    "    if month < 1:\n",
    "        month = 1\n",
    "    date = datetime(year,month,day)\n",
    "    return date\n",
    "    \n",
    "\n",
    "def feature_merge(data,train_data,feature):\n",
    "    train_gb = train_data.groupby(str(feature))\n",
    "    all_infos = {} \n",
    "    for key,value in train_gb:\n",
    "        info = {}\n",
    "        value = value[value['price']>0]\n",
    "        info[str(feature)+'_amount'] = len(value)\n",
    "        info[str(feature)+'_price_max'] = value.price.max()\n",
    "        info[str(feature)+'_price_min'] = value.price.min()\n",
    "        info[str(feature)+'_price_median'] = value.price.median()\n",
    "        info[str(feature)+'_price_mean'] = value.price.mean()\n",
    "        info[str(feature)+'_price_std'] = value.price.std() \n",
    "        all_infos[key] = info   \n",
    "    df = pd.DataFrame(all_infos).T.reset_index().rename(columns={'index':str(feature)})\n",
    "    data = data.merge(df,how='left',on=str(feature))    \n",
    "    return data   \n",
    "\n",
    "def reduce_mem_usage(data):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = data.memory_usage().sum() \n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else:\n",
    "            data[col] = data[col].astype('category')\n",
    "    end_mem = data.memory_usage().sum() \n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tree_feature(train_data,test_data)\n",
    "\n",
    "test = data[data['train']==0].drop(columns=['price','train'],axis=1)\n",
    "X = data[data['train']==1].drop(columns=['price','train'],axis=1)\n",
    "y = data[data['train']==1]['price'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(model,X,y,test):\n",
    "    n_folds = 5\n",
    "    kf=KFold(n_splits=n_folds,shuffle=True,random_state=88)\n",
    "    \n",
    "#     pred_val = np.zeros(len(X))\n",
    "    pred_test = np.zeros(len(test)) \n",
    "    mae_val = 0\n",
    "\n",
    "    \n",
    "    for fold,(train_idx,val_idx) in enumerate(kf.split(X,y)):\n",
    "        print('catb fold {}'.format(fold + 1))\n",
    "        x_train = X.iloc[train_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        x_val = X.iloc[val_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(x_train,y_train,\n",
    "                 eval_set=[(x_val,y_val)],\n",
    "                 verbose=1000)\n",
    "        \n",
    "        pred = model.predict(x_val)\n",
    "        mae_val += mean_absolute_error(np.expm1(pred),np.expm1(y_val))/kf.n_splits\n",
    "        \n",
    "        pred_test +=  model.predict(test)/kf.n_splits\n",
    "\n",
    "    return mae_val,np.expm1(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_folds = 5,learning_rate=0.03,max_depth=6,实际预测评分437\n",
    "\n",
    "cat_clf = CatBoostRegressor(\n",
    "                iterations=20000,\n",
    "                learning_rate=0.03,\n",
    "                depth=6,\n",
    "                task_type='CPU',\n",
    "                loss_function='MAE',\n",
    "                eval_metric='MAE',\n",
    "                od_type='Iter',\n",
    "                use_best_model=True,\n",
    "                early_stopping_rounds=50,\n",
    "                random_seed=88)\n",
    "\n",
    "mae_cat,prediction = k_folds(cat_clf,X,y,test)\n",
    "\n",
    "print('catboost mae:{:<8.8f}'.format(mae_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_catb = pd.DataFrame()\n",
    "result_catb['SaleID'] = test_data.SaleID\n",
    "result_catb['price'] = prediction\n",
    "result_catb['price'] = result_catb['price'].astype(np.int64)\n",
    "\n",
    "result_catb.to_csv('./result/catb_pred.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
